{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "âœ“ All functions loaded. To run training, use the cell below with sys.argv setup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, argparse, numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from spft.api import SPFTConfig, get_spft_model\n",
    "except ImportError:\n",
    "    print(\"WARNING: `spft` library not found. --mode sparselora will fail.\")\n",
    "    print(\"Please install with: pip install spft\")\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--dataset\", type=str, required=True, choices=[\"sst2\",\"imdb\",\"wikitext2\"])\n",
    "    p.add_argument(\"--model_name\", type=str, default=\"distilbert-base-uncased\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"outputs\")\n",
    "    # LoRA\n",
    "    p.add_argument(\"--r\", type=int, default=8)\n",
    "    p.add_argument(\"--alpha\", type=int, default=16)\n",
    "    p.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    # train\n",
    "    p.add_argument(\"--epochs\", type=int, default=3)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--bsz\", type=int, default=32)\n",
    "    p.add_argument(\"--seed\", type=int, default=685)\n",
    "    p.add_argument(\"--block_size\", type=int, default=256)\n",
    "    \n",
    "    # SparseLoRA\n",
    "    p.add_argument(\"--mode\", type=str, default=\"sparselora\", choices=[\"lora\", \"l1\", \"prune\", \"sparselora\"])\n",
    "    \n",
    "    p.add_argument(\"--lambda_l1\", type=float, default=0.0)\n",
    "    p.add_argument(\"--prune_amount\", type=float, default=0.5)\n",
    "\n",
    "    # Add argument for SPFT config file\n",
    "    p.add_argument(\"--spft_config_file\", type=str, default=None, \n",
    "                     help=\"Path to the .yaml config file for SparseLoRA (spft).\")\n",
    "    \n",
    "    return p.parse_args()\n",
    "\n",
    "def target_modules_for_distilbert():\n",
    "    # return [\"q_lin\",\"v_lin\"]\n",
    "    return [\"q_lin\",\"k_lin\",\"v_lin\",\"out_lin\"]\n",
    "def target_modules_for_gpt2():\n",
    "    return [\"c_attn\",\"c_fc\",\"c_proj\"]\n",
    "\n",
    "def load_cls_dataset(name, tok):\n",
    "    if name == \"sst2\":\n",
    "        raw = load_dataset(\"glue\", \"sst2\")\n",
    "        def _tok(ex): return tok(ex[\"sentence\"], truncation=True)\n",
    "        cols_remove = [\"sentence\"]\n",
    "        label_col = \"label\"\n",
    "    elif name == \"imdb\":\n",
    "        raw = load_dataset(\"imdb\")\n",
    "        def _tok(ex): return tok(ex[\"text\"], truncation=True)\n",
    "        cols_remove = [\"text\"]\n",
    "        label_col = \"label\"\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "    tokenized = raw.map(_tok, batched=True, remove_columns=cols_remove)\n",
    "    return tokenized, label_col\n",
    "\n",
    "def load_wikitext2(tok, block_size=256):\n",
    "    from datasets import load_dataset\n",
    "    try:\n",
    "        raw = load_dataset(\"mindchain/wikitext2\")\n",
    "        print(\"Loaded dataset: mindchain/wikitext2\")\n",
    "    except Exception:\n",
    "        print(\"mindchain/wikitext2 not found, fallback to wikitext/wikitext-2-raw-v1\")\n",
    "        raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    def tokenize(examples):\n",
    "        out = tok(examples[\"text\"])\n",
    "        return {\"input_ids\": out[\"input_ids\"]}\n",
    "\n",
    "    tokenized = raw.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "    def group_texts(examples):\n",
    "        concatenated_ids = sum(examples[\"input_ids\"], [])\n",
    "        total_len = (len(concatenated_ids) // block_size) * block_size\n",
    "        if total_len == 0:\n",
    "            return {\"input_ids\": []}\n",
    "        result = {\n",
    "            \"input_ids\": [\n",
    "                concatenated_ids[i : i + block_size]\n",
    "                for i in range(0, total_len, block_size)\n",
    "            ]\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    chunked = tokenized.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        remove_columns=tokenized[\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    return chunked\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "def collect_lora_params(model):\n",
    "    return [p for n,p in model.named_parameters() if (\"lora_A\" in n or \"lora_B\" in n)]\n",
    "\n",
    "class L1Trainer(Trainer):\n",
    "    def __init__(self, *args, lambda_l1=0.0, **kw):\n",
    "        super().__init__(*args, **kw)\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self._lora_params = collect_lora_params(self.model)\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        out = model(**inputs)\n",
    "        loss = out.loss\n",
    "        if self.lambda_l1 > 0:\n",
    "            l1 = sum(p.abs().sum() for p in self._lora_params)\n",
    "            loss = loss + self.lambda_l1 * l1\n",
    "        return (loss, out) if return_outputs else loss\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    np.random.seed(args.seed); torch.manual_seed(args.seed)\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    results_csv = os.path.join(args.output_dir, \"results_baseline.csv\")\n",
    "    if not os.path.exists(results_csv):\n",
    "        with open(results_csv, \"w\") as f:\n",
    "            f.write(\"dataset,model,adapter,r,alpha,dropout,epochs,lr,trainable_params,total_params,accuracy,train_time_min,peak_vram_gb,notes\\n\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
    "# train for sst2 and imdb\n",
    "    if args.dataset in (\"sst2\", \"imdb\"):\n",
    "        ds, label_col = load_cls_dataset(args.dataset, tokenizer)\n",
    "        collator = DataCollatorWithPadding(tokenizer)\n",
    "        num_labels = len(set(ds[\"train\"][label_col]))\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            args.model_name, num_labels=num_labels\n",
    "        )\n",
    "\n",
    "        # LoRA for DistilBERT\n",
    "        if args.mode == \"lora\":\n",
    "            lora_cfg = LoraConfig(\n",
    "                r=args.r,\n",
    "                lora_alpha=args.alpha,\n",
    "                lora_dropout=args.dropout,\n",
    "                target_modules=target_modules_for_distilbert(),\n",
    "                bias=\"none\",\n",
    "                task_type=\"SEQ_CLS\",\n",
    "            )\n",
    "            model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "        # Apply SparseLoRA patches \n",
    "        if args.mode == \"sparselora\":\n",
    "            if not args.spft_config_file:\n",
    "                raise ValueError(\"--spft_config_file is required for --mode sparselora\")\n",
    "            print(f\"Applying SparseLoRA (SPFT) patches from: {args.spft_config_file}\")\n",
    "            spft_config = SPFTConfig.from_file(args.spft_config_file)\n",
    "            model = get_spft_model(model, spft_config)\n",
    "\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            preds = np.argmax(logits, axis=-1)\n",
    "            return {\n",
    "                \"accuracy\": metric.compute(\n",
    "                    predictions=preds, references=labels\n",
    "                )[\"accuracy\"]\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            TrainingArguments(evaluation_strategy=\"epoch\")\n",
    "            eval_kw = {\"evaluation_strategy\": \"epoch\"}\n",
    "        except TypeError:\n",
    "            eval_kw = {\"eval_strategy\": \"epoch\"}\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(args.output_dir, f\"{args.dataset}_{args.mode}\"),\n",
    "            per_device_train_batch_size=args.bsz,\n",
    "            per_device_eval_batch_size=args.bsz,\n",
    "            learning_rate=args.lr,\n",
    "            num_train_epochs=args.epochs,\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            seed=args.seed,\n",
    "            **eval_kw,\n",
    "        )\n",
    "        \n",
    "        # \"sparselora\" is not \"l1\", so it will use Trainer.\n",
    "        TrainerCls = Trainer if args.mode != \"l1\" else L1Trainer\n",
    "        trainer = TrainerCls(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds[\"train\"],\n",
    "            eval_dataset=ds[\"validation\"] if args.dataset == \"sst2\" else ds[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            **({\"lambda_l1\": args.lambda_l1} if args.mode == \"l1\" else {}),\n",
    "        )\n",
    "#train for wikitext2\n",
    "    else:\n",
    "        if args.model_name == \"distilbert-base-uncased\":\n",
    "            args.model_name = \"distilgpt2\"\n",
    "        from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        ds = load_wikitext2(tokenizer, block_size=args.block_size)\n",
    "        collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        if args.mode == \"lora\":\n",
    "            lora_cfg = LoraConfig(\n",
    "                r=args.r,\n",
    "                lora_alpha=args.alpha,\n",
    "                lora_dropout=args.dropout,\n",
    "                target_modules=target_modules_for_gpt2(),\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "            )\n",
    "            model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "        # apply SparseLoRA patches \n",
    "        if args.mode == \"sparselora\":\n",
    "            if not args.spft_config_file:\n",
    "                raise ValueError(\"--spft_config_file is required for --mode sparselora\")\n",
    "            print(f\"Applying SparseLoRA (SPFT) patches from: {args.spft_config_file}\")\n",
    "            # Load the SPFT config file\n",
    "            spft_config = SPFTConfig.from_file(args.spft_config_file)\n",
    "            model = get_spft_model(model, spft_config)\n",
    "\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        try:\n",
    "            TrainingArguments(evaluation_strategy=\"epoch\")\n",
    "            eval_kw = {\"evaluation_strategy\": \"epoch\"}\n",
    "        except TypeError:\n",
    "            eval_kw = {\"eval_strategy\": \"epoch\"}\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(args.output_dir, f\"{args.dataset}_{args.mode}\"),\n",
    "            per_device_train_batch_size=args.bsz,\n",
    "            per_device_eval_batch_size=args.bsz,\n",
    "            learning_rate=args.lr,\n",
    "            num_train_epochs=args.epochs,\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"loss\", \n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to=\"none\",\n",
    "            seed=args.seed,\n",
    "            **eval_kw,\n",
    "        )\n",
    "\n",
    "        TrainerCls = Trainer if args.mode != \"l1\" else L1Trainer\n",
    "        trainer = TrainerCls(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds[\"train\"],\n",
    "            eval_dataset=ds.get(\"validation\", ds.get(\"test\")),\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "            **({\"lambda_l1\": args.lambda_l1} if args.mode == \"l1\" else {}),\n",
    "        )\n",
    "\n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    trainer.train()\n",
    "    train_time_min = (time.time() - start) / 60.0\n",
    "    peak_vram_gb = (\n",
    "        torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        if torch.cuda.is_available()\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    if args.mode == \"prune\":\n",
    "        for mod_name, mod in model.named_modules():\n",
    "            for pname, _ in list(mod.named_parameters(recurse=False)):\n",
    "                if pname in (\"lora_A.weight\", \"lora_B.weight\"):\n",
    "                    prune.l1_unstructured(mod, name=pname, amount=args.prune_amount)\n",
    "                    prune.remove(mod, pname)\n",
    "\n",
    "    adapter_dir = os.path.join(training_args.output_dir, f\"adapter_{args.mode}\")\n",
    "    model.save_pretrained(adapter_dir)\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    if args.dataset in (\"sst2\", \"imdb\"):\n",
    "        acc_or_ppl = metrics.get(\"eval_accuracy\", float(\"nan\"))\n",
    "    else:\n",
    "        import math\n",
    "\n",
    "        eval_loss = metrics[\"eval_loss\"]\n",
    "        acc_or_ppl = math.exp(min(20, eval_loss))\n",
    "\n",
    "    trainable, total = count_trainable_params(model)\n",
    "    with open(os.path.join(args.output_dir, \"results_baseline.csv\"), \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{args.dataset},{args.model_name},{args.mode},{args.r},{args.alpha},{args.dropout},\"\n",
    "            f\"{args.epochs},{args.lr},{trainable},{total},{acc_or_ppl:.4f},{train_time_min:.2f},{peak_vram_gb:.2f},baseline\\n\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nRESULT:\",\n",
    "        f\"{args.dataset},{args.model_name},{args.mode},{args.r},{args.alpha},{args.dropout},\"\n",
    "        f\"{args.epochs},{args.lr},{trainable},{total},{acc_or_ppl:.4f},{train_time_min:.2f},{peak_vram_gb:.2f},baseline\",\n",
    "    )\n",
    "    print(\"Adapter saved to:\", adapter_dir)\n",
    "\n",
    "# Only run main() if not in a Jupyter/IPython environment\n",
    "# In notebooks, call main() manually with sys.argv or create an args object\n",
    "try:\n",
    "    get_ipython()\n",
    "    # We're in IPython/Jupyter - don't auto-run\n",
    "    print(\"âœ“ All functions loaded. To run training, use the cell below with sys.argv setup.\")\n",
    "except NameError:\n",
    "    # We're in a regular Python script - run normally\n",
    "    if __name__ == \"__main__\":\n",
    "        main() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74caac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SparseLoRA (SPFT) patches from: sparselora_config.yaml\n",
      "[Init] Setting start_step to 0.05\n",
      "[Init] Setting end_step to 1.0\n",
      "[Init] Setting mode to svd_8\n",
      "[Init] Setting skip_output_tokens to True\n",
      "[Init] Setting skip_sink_tokens to 0\n",
      "[Init] Setting skip_random_tokens to False\n",
      "[Init] Setting sparse_output_tokens to 0.0\n",
      "[Init] Setting sparse_lora_branch to False\n",
      "Patching SparseLoRA onto HF model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mðŸš€ Patching Model --> Fast Fine-tuning!\u001b[0m: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [00:00<00:00, 393144.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/7_0g4ktd059gy5sk4pcpjklh0000gn/T/ipykernel_76170/1316612616.py:200: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerCls(\n",
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 06:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>0.896789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.281608</td>\n",
       "      <td>0.901376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.311017</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT: sst2,distilbert-base-uncased,sparselora,8,16,0.1,3,0.0002,887042,67842052,0.9037,6.62,0.00,baseline\n",
      "Adapter saved to: outputs/sst2_sparselora/adapter_sparselora\n"
     ]
    }
   ],
   "source": [
    "# Example: sst2\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'script.py',\n",
    "    '--dataset', 'sst2',           # or 'imdb', 'wikitext2'\n",
    "    '--mode', 'sparselora',              # or 'l1', 'prune', 'lora'\n",
    "    '--model_name', 'distilbert-base-uncased',\n",
    "    '--output_dir', 'outputs',\n",
    "    '--r', '8',\n",
    "    '--alpha', '16',\n",
    "    '--dropout', '0.1',\n",
    "    '--epochs', '3',\n",
    "    '--lr', '2e-4',\n",
    "    '--bsz', '32',\n",
    "    '--seed', '685',\n",
    "    # For sparselora mode, also add:\n",
    "    '--spft_config_file', 'sparselora_config.yaml',\n",
    "]\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93951db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9199419298249c5b9b0508fdbc86018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d4d20e01534d9caf6773ee82514d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b30cb0c0c84e70872af4f4fd6f01b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5276a911ea2e4cb2acdf4c9d88db1497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f58a6e11339417790c2412bb29303d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d2d9473d574dc6acf2925315451776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3b5f127a9c411eaf2584cefd31f6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3404a1cf784a2f8cd8e767291e5a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6865be7fc34e59bcfc7f77e81908a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1377948ab844e71bccb4863e80e1040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SparseLoRA (SPFT) patches from: sparselora_config.yaml\n",
      "[Init] Setting start_step to 0.05\n",
      "[Init] Setting end_step to 1.0\n",
      "[Init] Setting mode to svd_8\n",
      "[Init] Setting skip_output_tokens to True\n",
      "[Init] Setting skip_sink_tokens to 0\n",
      "[Init] Setting skip_random_tokens to False\n",
      "[Init] Setting sparse_output_tokens to 0.0\n",
      "[Init] Setting sparse_lora_branch to False\n",
      "Patching SparseLoRA onto HF model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mðŸš€ Patching Model --> Fast Fine-tuning!\u001b[0m: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [00:00<00:00, 417851.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/25/7_0g4ktd059gy5sk4pcpjklh0000gn/T/ipykernel_76170/1316612616.py:200: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerCls(\n",
      "/Users/maggie-z/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='783' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 783/2346 13:40 < 27:22, 0.95 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/782 01:13 < 04:03, 2.46 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: imdb\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.argv = [\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscript.py\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m--dataset\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mimdb\u001b[39m\u001b[33m'\u001b[39m,          \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m--spft_config_file\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msparselora_config.yaml\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 282\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m    281\u001b[39m     torch.cuda.reset_peak_memory_stats()\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m train_time_min = (time.time() - start) / \u001b[32m60.0\u001b[39m\n\u001b[32m    284\u001b[39m peak_vram_gb = (\n\u001b[32m    285\u001b[39m     torch.cuda.max_memory_allocated() / (\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m)\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available()\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    288\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:2790\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2795\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2796\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:3221\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3219\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3221\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3222\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:3170\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3170\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3171\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3173\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:4489\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4486\u001b[39m start_time = time.time()\n\u001b[32m   4488\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4489\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4490\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4492\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4499\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/transformers/trainer.py:4707\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4705\u001b[39m     labels = \u001b[38;5;28mself\u001b[39m.accelerator.pad_across_processes(labels, dim=\u001b[32m1\u001b[39m, pad_index=-\u001b[32m100\u001b[39m)\n\u001b[32m   4706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4707\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.preprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4709\u001b[39m         logits = \u001b[38;5;28mself\u001b[39m.preprocess_logits_for_metrics(logits, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/accelerate/accelerator.py:3100\u001b[39m, in \u001b[36mAccelerator.pad_across_processes\u001b[39m\u001b[34m(self, tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m   3067\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim=\u001b[32m0\u001b[39m, pad_index=\u001b[32m0\u001b[39m, pad_first=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   3068\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3069\u001b[39m \u001b[33;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[32m   3070\u001b[39m \u001b[33;03m    they can safely be gathered.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3098\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   3099\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/accelerate/utils/operations.py:408\u001b[39m, in \u001b[36mchained_operation.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    410\u001b[39m         operation = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/accelerate/utils/operations.py:678\u001b[39m, in \u001b[36mpad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    675\u001b[39m     new_tensor[indices] = tensor\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/accelerate/utils/operations.py:127\u001b[39m, in \u001b[36mrecursively_apply\u001b[39m\u001b[34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[32m    119\u001b[39m         {\n\u001b[32m    120\u001b[39m             k: recursively_apply(\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m         }\n\u001b[32m    125\u001b[39m     )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. Only nested list/tuple/dicts of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` should be passed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Github/ECE658_LORA/venv_sparselora/lib/python3.12/site-packages/accelerate/utils/operations.py:658\u001b[39m, in \u001b[36mpad_across_processes.<locals>._pad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    655\u001b[39m     dim += \u001b[38;5;28mlen\u001b[39m(tensor.shape)\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m size = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    659\u001b[39m sizes = gather(size).cpu()\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example: imdb\n",
    "sys.argv = [\n",
    "    'script.py',\n",
    "    '--dataset', 'imdb',          \n",
    "    '--mode', 'sparselora',              # or 'l1', 'prune', 'lora'\n",
    "    '--model_name', 'distilbert-base-uncased',\n",
    "    '--output_dir', 'outputs',\n",
    "    '--r', '8',\n",
    "    '--alpha', '16',\n",
    "    '--dropout', '0.1',\n",
    "    '--epochs', '3',\n",
    "    '--lr', '2e-4',\n",
    "    '--bsz', '32',\n",
    "    '--seed', '685',\n",
    "    # For sparselora mode, also add:\n",
    "    '--spft_config_file', 'sparselora_config.yaml',\n",
    "]\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338fc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: wikitext2\n",
    "sys.argv = [\n",
    "    'script.py',\n",
    "    '--dataset', 'wikitext2',          \n",
    "    '--mode', 'sparselora',              # or 'l1', 'prune', 'lora'\n",
    "    '--model_name', 'distilbert-base-uncased',\n",
    "    '--output_dir', 'outputs',\n",
    "    '--r', '8',\n",
    "    '--alpha', '16',\n",
    "    '--dropout', '0.1',\n",
    "    '--epochs', '3',\n",
    "    '--lr', '2e-4',\n",
    "    '--bsz', '32',\n",
    "    '--seed', '685',\n",
    "    # For sparselora mode, also add:\n",
    "    '--spft_config_file', 'sparselora_config.yaml',\n",
    "]\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sparselora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
